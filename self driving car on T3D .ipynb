{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training self driving car on TD3 deep reinforcement learning algorithm \n",
    "\n",
    "## Summary of what is done \n",
    "- Added CNN to calculate state from image\n",
    "    - replaced the sensor input with cropped & rotated Image input\n",
    "- Inputting the orientation of the car to neural network\n",
    "    - __NOTE__ : Please refer to Actor and Critic Images to understand better\n",
    "    - Without orientation car was stumbling here and therewhile staying on road. \n",
    "    - So added orientation to acknowledge the agent to reach destiny\n",
    "- Shifted the entire update operation to car.py from brain.update()\n",
    "    \n",
    "    \n",
    "__NOTE__ : I still kept some of the old code commented without deleting. So that reader can find it easy to correlate the old code with the new one\n",
    "\n",
    "## Observations\n",
    "- After training with this code car was able to understand how to keep itself on roads. But was unable to learn how to reach destiny\n",
    "- tweaking the reward and environment and done conditions should give better results\\\n",
    "__NOTE__ : I dont have GPU so I was unable to do much hyper parameter tuning\n",
    "\n",
    "\n",
    "\n",
    "## Improvements (that can be done)\n",
    "- tweak the parameters\n",
    "\n",
    "\n",
    "\n",
    "__Refer__ [this](https://youtu.be/2h8b4orhTT4) link to see some video of how car was training.\n",
    "- These are some small instances of recording while the model was training.\n",
    "- It can be clearly observed that the model is trying to stay on road while not very confident about reaching the destination\n",
    "\n",
    "__NOTE__ : I have recorded the video before removing the sensor image (dots infront of car) .Apologies for that\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Experience Replay\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, max_size=6e2):\n",
    "    self.storage = []\n",
    "    self.max_size = max_size\n",
    "    self.ptr = 0\n",
    "\n",
    "  def add(self, transition):\n",
    "    if len(self.storage) == self.max_size:\n",
    "      self.storage[int(self.ptr)] = transition\n",
    "      self.ptr = (self.ptr + 1) % self.max_size\n",
    "    else:\n",
    "      self.storage.append(transition)\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "    batch_states, batch_orientation , batch_next_states, batch_next_orientation,  batch_actions, batch_rewards, batch_dones = [], [], [], [], [], [], []\n",
    "    for i in ind:\n",
    "      state, orientation,  next_state, next_orientation, action, reward, done = self.storage[i]\n",
    "      batch_states.append(np.array(state, copy=False))\n",
    "        \n",
    "      ### NEW\n",
    "      batch_orientation.append(np.array(orientation, copy=False))\n",
    "      ### NEW\n",
    "    \n",
    "      batch_next_states.append(np.array(next_state, copy=False))\n",
    "        \n",
    "      ### NEW\n",
    "      batch_next_orientation.append(np.array(next_orientation, copy=False))\n",
    "      ### NEW\n",
    "    \n",
    "      batch_actions.append(np.array(action, copy=False))\n",
    "      batch_rewards.append(np.array(reward, copy=False))\n",
    "      batch_dones.append(np.array(done, copy=False))\n",
    "    return np.array(batch_states), np.array(batch_orientation),np.array(batch_next_states), np.array(batch_next_orientation), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## actor architecture\n",
    "![actor](image_pres/final_actor.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.convblock1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=0, bias=False, stride = 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.convblock2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16)\n",
    "    )\n",
    "    self.pool1 = nn.MaxPool2d(2, 2)\n",
    "    self.convblock3 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16)\n",
    "    )\n",
    "    #self.pool2 = nn.MaxPool2d(2,2)\n",
    "    self.convblock4 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.pool3 = nn.MaxPool2d(2,2)\n",
    "    self.convblock5 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.convblock6 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=2, kernel_size=(1, 1), padding=0),\n",
    "        nn.BatchNorm2d(2),\n",
    "        # nn.ReLU() NEVER!\n",
    "    )\n",
    "    self.layer_1 = nn.Linear(18 + 2, 16)\n",
    "    self.layer_2 = nn.Linear(16, 8)\n",
    "    self.layer_3 = nn.Linear(8, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "\n",
    "  def forward(self, x, o):\n",
    "    #input channels shape 60*60*1\n",
    "    x = self.convblock1(x) #30*30*8(because of stride = 2)\n",
    "    x = self.convblock2(x) #28*28*16\n",
    "    x = self.pool1(x)      #14*14*16\n",
    "    #x = self.convblock3(x)\n",
    "    #x = self.pool2(x)\n",
    "    x = self.convblock4(x) #11*11*8\n",
    "    x = self.pool3(x)      #5*5*8\n",
    "    x = self.convblock5(x) #3*3*8\n",
    "    x = self.convblock6(x) #3*3*2\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = torch.cat([x, o], 1)\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## critic architecture\n",
    "![critic](image_pres/final_critic.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # Defining the first Critic neural network\n",
    "\n",
    "    self.convblock1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=0, bias=False, stride = 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.convblock2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16)\n",
    "    )\n",
    "    self.pool1 = nn.MaxPool2d(2, 2)\n",
    "    self.convblock3 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16)\n",
    "    )\n",
    "    self.pool2 = nn.MaxPool2d(2, 2)\n",
    "    self.convblock4 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.pool3 = nn.MaxPool2d(2,2)\n",
    "    self.convblock5 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.convblock6 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=2, kernel_size=(1, 1), padding=0,),\n",
    "        nn.BatchNorm2d(2)\n",
    "        # nn.ReLU() NEVER!\n",
    "    )\n",
    "\n",
    "\n",
    "    self.convblock7 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=0, bias=False, stride = 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.convblock8 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16)\n",
    "    )\n",
    "    self.pool4 = nn.MaxPool2d(2, 2)\n",
    "    self.convblock9 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16)\n",
    "    )\n",
    "    self.pool5 = nn.MaxPool2d(2, 2)\n",
    "    self.convblock10 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.pool6 = nn.MaxPool2d(2,2)\n",
    "    self.convblock11 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.convblock12 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=2, kernel_size=(1, 1), padding=0),\n",
    "        nn.BatchNorm2d(2)\n",
    "        # nn.ReLU() NEVER!\n",
    "    )\n",
    "\n",
    "    self.layer_1 = nn.Linear(18+2+action_dim, 16)\n",
    "    self.layer_2 = nn.Linear(16, 8)\n",
    "    self.layer_3 = nn.Linear(8, 1)\n",
    "    # Defining the second Critic neural network\n",
    "    self.layer_4 = nn.Linear(18+2+action_dim, 16)\n",
    "    self.layer_5 = nn.Linear(16, 8)\n",
    "    self.layer_6 = nn.Linear(8, 1)\n",
    "\n",
    "  def forward(self, x, o, u):\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    #input shape 60*60*1\n",
    "    x1 = self.convblock1(x)   #30*30*8\n",
    "    x1 = self.convblock2(x1)  #28*28*16\n",
    "    x1 = self.pool1(x1)       #14*14*16\n",
    "    x1 = self.convblock4(x1)  #11*11*8\n",
    "    x1 = self.pool3(x1)       #5*5*8\n",
    "    x1 = self.convblock5(x1)  #3*3*8\n",
    "    x1 = self.convblock6(x1)  #3*3*2\n",
    "    x1 = x1.view( x1.size(0), -1)\n",
    "    x1o = torch.cat([x1,o], 1)\n",
    "    x1u = torch.cat([x1o, u], 1)\n",
    "    x1 = F.relu(self.layer_1(x1u))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    \n",
    "    # Forward-Propagation on the second Critic Neural Network\n",
    "    x2 = self.convblock7(x)   #30*30*8\n",
    "    x2 = self.convblock8(x2)  #28*28*16\n",
    "    x2 = self.pool4(x2)       #14*14*16\n",
    "    x2 = self.convblock10(x2) #11*11*8\n",
    "    x2 = self.pool6(x2)       #5*5*8\n",
    "    x2 = self.convblock11(x2) #3*3*8\n",
    "    x2 = self.convblock12(x2) #3*3*2\n",
    "    x2 = x2.view( x2.size(0), -1)\n",
    "    x2o = torch.cat([x2,o], 1)\n",
    "    x2u = torch.cat([x2o, u], 1)\n",
    "    x2 = F.relu(self.layer_4(x2u))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, o, u):\n",
    "    x1 = self.convblock1(x)\n",
    "    x1 = self.convblock2(x1)\n",
    "    x1 = self.pool1(x1)\n",
    "    #x1 = self.convblock3(x1)\n",
    "    x1 = self.convblock4(x1)\n",
    "    x1 = self.pool1(x1)\n",
    "    x1 = self.convblock5(x1)\n",
    "    x1 = self.convblock6(x1)\n",
    "    x1 = x1.view( x1.size(0), -1)\n",
    "    x1o = torch.cat([x1,o], 1)\n",
    "    x1u = torch.cat([x1o, u], 1)\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(x1u))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "\n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state, orientation):\n",
    "    # state input for list state\n",
    "    #state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "\n",
    "    # state input for image state ### NEW\n",
    "    state = torch.Tensor(state).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    orientation = torch.Tensor(orientation).unsqueeze(0).to(device)\n",
    "    return self.actor(state, orientation).cpu().data.numpy().flatten()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "\n",
    "    for it in range(iterations):\n",
    "\n",
    "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "      batch_states, batch_orientation, batch_next_states, batch_next_orientation, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "        \n",
    "      #for cnn state calc       ### NEW\n",
    "      state = torch.Tensor(batch_states).unsqueeze(1).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).unsqueeze(1).to(device)\n",
    "      # for cnn state calc      ### NEW\n",
    "\n",
    "      #for sensor state calc\n",
    "      # state = torch.Tensor(batch_states).to(device)\n",
    "      # next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      # for sensor state calc\n",
    "    \n",
    "      # added orientataion     ### NEW\n",
    "      orientation = torch.Tensor(batch_orientation).to(device)\n",
    "      next_orientation = torch.Tensor(batch_next_orientation).to(device)\n",
    "      # added orientation      ### NEW\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "\n",
    "\n",
    "\n",
    "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "      next_action = self.actor_target(next_state, next_orientation)\n",
    "\n",
    "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, next_orientation,  next_action)\n",
    "\n",
    "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "      target_Q = torch.min(target_Q1, target_Q2)\n",
    "\n",
    "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "\n",
    "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, orientation, action)\n",
    "\n",
    "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "\n",
    "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "      if it % policy_freq == 0:\n",
    "        # actor_loss,_ = -self.critic(state, orientation, self.actor(state))\n",
    "        # actor_loss = actor_loss.mean()\n",
    "        #print(\"debug : \",state.shape, type(state), orientation.shape, type(orientation), self.actor(state).shape, type(self.actor(state)))\n",
    "        actor_loss = -self.critic.Q1(state, orientation, self.actor(state, orientation)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "  # Making a save method to save a trained model\n",
    "  def save(self, filename = \"temp\", directory = \"models\"):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename = \"temp\", directory = \"models\"):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Started KIWI environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Driving Car\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "from random import random, randint\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Importing the Kivy packages\n",
    "from kivy.app import App\n",
    "from kivy.uix.widget import Widget\n",
    "from kivy.uix.button import Button\n",
    "from kivy.graphics import Color, Ellipse, Line\n",
    "from kivy.config import Config\n",
    "from kivy.properties import NumericProperty, ReferenceListProperty, ObjectProperty\n",
    "from kivy.vector import Vector\n",
    "from kivy.clock import Clock\n",
    "from kivy.core.image import Image as CoreImage\n",
    "from PIL import Image as PILImage\n",
    "from kivy.graphics.texture import Texture\n",
    "\n",
    "# Importing the Dqn object from our AI in ai.py\n",
    "#from aiT3D import TD3, ReplayBuffer\n",
    "import random\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding this line if we don't want the right click to put a red point\n",
    "Config.set('input', 'mouse', 'mouse,multitouch_on_demand')\n",
    "Config.set('graphics', 'resizable', False)\n",
    "Config.set('graphics', 'width', '1429')\n",
    "Config.set('graphics', 'height', '660')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters START\n",
    "seed = 0 # Random seed number\n",
    "start_timesteps = 1e2 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "eval_freq = 5e2 # How often the evaluation step is performed (after how many timesteps)\n",
    "#max_timesteps = 5e5 # Total number of iterations/timesteps\n",
    "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
    "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
    "batch_size = 30 # Size of the batch\n",
    "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.005 # Target network update rate\n",
    "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated\n",
    "done = True\n",
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "\n",
    "action_len = 1\n",
    "state_len = 5\n",
    "last_time_steps = 1\n",
    "image_size = 60\n",
    "orientation = -0.9\n",
    "#obs = [0.23,1,1,0.5, -0.5]\n",
    "# model parameters END\n",
    "\n",
    "# model global params\n",
    "replay_buffer = ReplayBuffer()\n",
    "# model global params\n",
    "\n",
    "\n",
    "# Introducing last_x and last_y, used to keep the last point in memory when we draw the sand on the map\n",
    "last_x = 0\n",
    "last_y = 0\n",
    "n_points = 0\n",
    "length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our AI, which we call \"brain\", and that contains our neural network that represents our Q-function\n",
    "max_action_agent = 40\n",
    "brain = TD3(state_len,action_len,max_action_agent)\n",
    "action2rotation = [0,5,-5]\n",
    "reward = 0\n",
    "scores = []\n",
    "reward_window = []\n",
    "im = CoreImage(\"./images/MASK1.png\")\n",
    "main_img = cv2.imread('./images/mask.png',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textureMask = CoreImage(source=\"./kivytest/simplemask1.png\")\n",
    "def save_cropped_image(img, x, y, name = \"\"):\n",
    "    # print(\"entered\")\n",
    "    # data = np.array(img)# * 255.0\n",
    "    # rescaled = data.astype(np.uint8)\n",
    "    # im = Image.fromarray(rescaled)\n",
    "    # im.save(\"./check/\"+name+ \"_\" + \"your_file\"+str(x) +\"_\"+ str(y) +\".png\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to fetch cropped & rotated image\n",
    "\n",
    "- Let \"AxA\" the required shape\n",
    "- Crop the image in a shape of 1.414xA\n",
    "    - because the square_root 2 of side of a square == hypotenuse\n",
    "- Rotate the image\n",
    "- Again crop \"AxA\" shaped required image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_image(img, angle, center, size, fill_with = 255):\n",
    "    angle = angle + 90\n",
    "    center[0] -= 0\n",
    "    img = np.pad(img, size, 'constant', constant_values = fill_with)\n",
    "    init_size = 1.6*size ## this is because the square_root 2 of side of a square == hypotenuse\n",
    "    #print(img.shape)\n",
    "    center[0] += size\n",
    "    center[1] += size\n",
    "    #print(int(center[0]-(init_size/2)) , int(center[1]-(init_size/2)),int(center[0]+(init_size/2)) , int(center[1]+(init_size/2)))\n",
    "    cropped = img[int(center[0]-(init_size/2)) : int(center[0]+(init_size/2)) ,int(center[1]-(init_size/2)): int(center[1]+(init_size/2))]\n",
    "    rotated = ndimage.rotate(cropped, angle, reshape = False, cval = 255.0)\n",
    "    y,x = rotated.shape\n",
    "    final = rotated[int(y/2-(size/2)):int(y/2+(size/2)),int(x/2-(size/2)):int(x/2+(size/2))]\n",
    "    return cropped, rotated, final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the map\n",
    "first_update = True\n",
    "def init():\n",
    "    global sand\n",
    "    global goal_x\n",
    "    global goal_y\n",
    "    global first_update\n",
    "    sand = np.zeros((longueur,largeur))\n",
    "    img = PILImage.open(\"./images/mask.png\").convert('L')\n",
    "    sand = np.asarray(img)/255\n",
    "    goal_x = 1420\n",
    "    goal_y = 622\n",
    "    first_update = False\n",
    "    global swap\n",
    "    swap = 0\n",
    "\n",
    "\n",
    "# Initializing the last distance\n",
    "last_distance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the car class\n",
    "\n",
    "class Car(Widget):\n",
    "\n",
    "    angle = NumericProperty(0)\n",
    "    rotation = NumericProperty(0)\n",
    "    velocity_x = NumericProperty(0)\n",
    "    velocity_y = NumericProperty(0)\n",
    "    velocity = ReferenceListProperty(velocity_x, velocity_y)\n",
    "    sensor1_x = NumericProperty(0)\n",
    "    sensor1_y = NumericProperty(0)\n",
    "    sensor1 = ReferenceListProperty(sensor1_x, sensor1_y)\n",
    "    sensor2_x = NumericProperty(0)\n",
    "    sensor2_y = NumericProperty(0)\n",
    "    sensor2 = ReferenceListProperty(sensor2_x, sensor2_y)\n",
    "    sensor3_x = NumericProperty(0)\n",
    "    sensor3_y = NumericProperty(0)\n",
    "    sensor3 = ReferenceListProperty(sensor3_x, sensor3_y)\n",
    "    signal1 = NumericProperty(0)\n",
    "    signal2 = NumericProperty(0)\n",
    "    signal3 = NumericProperty(0)\n",
    "\n",
    "    def move(self, rotation):\n",
    "        self.pos = Vector(*self.velocity) + self.pos\n",
    "        self.rotation = rotation\n",
    "        self.angle = self.angle + self.rotation\n",
    "        #self.sensor1 = Vector(30, 0).rotate(self.angle) + self.pos\n",
    "        #self.sensor2 = Vector(30, 0).rotate((self.angle+30)%360) + self.pos\n",
    "        #self.sensor3 = Vector(30, 0).rotate((self.angle-30)%360) + self.pos\n",
    "        #self.signal1 = int(np.sum(sand[int(self.sensor1_x)-10:int(self.sensor1_x)+10, int(self.sensor1_y)-10:int(self.sensor1_y)+10]))/400.\n",
    "        #self.signal2 = int(np.sum(sand[int(self.sensor2_x)-10:int(self.sensor2_x)+10, int(self.sensor2_y)-10:int(self.sensor2_y)+10]))/400.\n",
    "        #self.signal3 = int(np.sum(sand[int(self.sensor3_x)-10:int(self.sensor3_x)+10, int(self.sensor3_y)-10:int(self.sensor3_y)+10]))/400.\n",
    "        # if self.sensor1_x>longueur-10 or self.sensor1_x<10 or self.sensor1_y>largeur-10 or self.sensor1_y<10:\n",
    "        #     self.signal1 = 10.\n",
    "        # if self.sensor2_x>longueur-10 or self.sensor2_x<10 or self.sensor2_y>largeur-10 or self.sensor2_y<10:\n",
    "        #     self.signal2 = 10.\n",
    "        # if self.sensor3_x>longueur-10 or self.sensor3_x<10 or self.sensor3_y>largeur-10 or self.sensor3_y<10:\n",
    "        #     self.signal3 = 10.\n",
    "        #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the game class\n",
    "\n",
    "class Game(Widget):\n",
    "\n",
    "    car = ObjectProperty(None)\n",
    "    ball1 = ObjectProperty(None)\n",
    "    ball2 = ObjectProperty(None)\n",
    "    ball3 = ObjectProperty(None)\n",
    "\n",
    "    def serve_car(self):\n",
    "        #my_rand_points = [((715, 360),0),((348,414),90),((127,350),95),((581,432),270),((882,71),20),((970,278),0)]\n",
    "        my_rand_points = [((715, 360),0)]\n",
    "        (x,y),angle = random.choice(my_rand_points)\n",
    "        self.car.center = (x,y)\n",
    "        self.car.angle = angle\n",
    "        self.car.velocity = Vector(4, 0)\n",
    "\n",
    "\n",
    "    def update(self, dt):\n",
    "\n",
    "        global brain\n",
    "        global reward\n",
    "        global scores\n",
    "        global last_distance\n",
    "        global goal_x\n",
    "        global goal_y\n",
    "        global longueur\n",
    "        global largeur\n",
    "        global swap\n",
    "        global orientation\n",
    "\n",
    "\n",
    "        global obs\n",
    "\n",
    "\n",
    "        # NEW GLOBALS\n",
    "        global replay_buffer\n",
    "        global seed\n",
    "        global start_timesteps\n",
    "        global eval_freq\n",
    "        #global max_timesteps\n",
    "        global save_models\n",
    "        global expl_noise\n",
    "        global batch_size\n",
    "        global discount\n",
    "        global tau\n",
    "        global policy_noise\n",
    "        global noise_clip\n",
    "        global policy_freq\n",
    "        global done\n",
    "        global total_timesteps\n",
    "        global timesteps_since_eval\n",
    "        global episode_num\n",
    "        global episode_reward\n",
    "        global reward_window\n",
    "\n",
    "        global episode_timesteps\n",
    "        global main_img\n",
    "        global image_size\n",
    "\n",
    "        global last_time_steps\n",
    "        # NEW GLOBALS\n",
    "\n",
    "\n",
    "        longueur = self.width\n",
    "        largeur = self.height\n",
    "        if first_update:\n",
    "            init()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #if total_timesteps < max_timesteps:\n",
    "        if True :\n",
    "\n",
    "          # If the episode is done\n",
    "          if done:\n",
    "\n",
    "\n",
    "            # If we are not at the very beginning, we start the training process of the model\n",
    "            if total_timesteps != 0:\n",
    "              print(\"Total Timesteps: {} Episode Num: {} Timesteps diff: {} Reward: {} score: {}\".format(total_timesteps, episode_num, total_timesteps - last_time_steps,episode_reward, episode_reward/(total_timesteps - last_time_steps)))\n",
    "              brain.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "              last_time_steps = total_timesteps\n",
    "\n",
    "            # We evaluate the episode and we save the policy\n",
    "            # WILL COME TO THIS LATER\n",
    "            # if timesteps_since_eval >= eval_freq:\n",
    "            #   timesteps_since_eval %= eval_freq\n",
    "            #   evaluations.append(evaluate_policy(policy))\n",
    "            #   policy.save(file_name, directory=\"./pytorch_models\")\n",
    "            #   np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "\n",
    "            # state calculation\n",
    "            # self.serve_car()\n",
    "            # xx = goal_x - self.car.x\n",
    "            # yy = goal_y - self.car.y\n",
    "            # orientation = Vector(*self.car.velocity).angle((xx,yy))/180.\n",
    "            # obs = [self.car.signal1, self.car.signal2, self.car.signal3, orientation, -orientation]\n",
    "            # state calculation\n",
    "\n",
    "            #cnn state calculation   ### NEW\n",
    "            self.serve_car()\n",
    "            _,_,obs = get_target_image(main_img, self.car.angle, [self.car.x, self.car.y], image_size)\n",
    "            # save_cropped_image(obs, self.car.x, self.car.y, name = \"initial\")\n",
    "            \n",
    "            xx = goal_x - self.car.x\n",
    "            yy = goal_y - self.car.y\n",
    "            orientation = Vector(*self.car.velocity).angle((xx,yy))/180.\n",
    "            orientation = [orientation, -orientation]\n",
    "\n",
    "            #cnn state calculation  ### NEW\n",
    "\n",
    "            # When the training step is done, we reset the state of the environment\n",
    "            # obs = env.reset()\n",
    "\n",
    "            # Set the Done to False\n",
    "            done = False\n",
    "            # Set rewards and episode timesteps to zero\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1\n",
    "\n",
    "          # Before 10000 timesteps, we play random actions\n",
    "          if total_timesteps < start_timesteps:\n",
    "            action = [random.uniform(-max_action_agent * 1.0, max_action_agent * 1.0)]\n",
    "            #action = env.action_space.sample()\n",
    "          else: # After 10000 timesteps, we switch to the model\n",
    "            action = brain.select_action(np.array(obs), np.array(orientation))\n",
    "            # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "            if expl_noise != 0:\n",
    "              action = (action + np.random.normal(0, expl_noise, size=action_len)).clip(-1*max_action_agent,max_action_agent)\n",
    "\n",
    "          # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
    "\n",
    "\n",
    "          # ENV STEP PERFORM START\n",
    "          if type(action) != type([]):\n",
    "              #print(\"action : \",type(action.tolist()[0]), type(action[0]))\n",
    "              self.car.move(action.tolist()[0])\n",
    "          else:\n",
    "              self.car.move(action[0])\n",
    "          distance = np.sqrt((self.car.x - goal_x)**2 + (self.car.y - goal_y)**2)\n",
    "          # self.ball1.pos = self.car.sensor1\n",
    "          # self.ball2.pos = self.car.sensor2\n",
    "          # self.ball3.pos = self.car.sensor3\n",
    "\n",
    "          if sand[int(self.car.x),int(self.car.y)] > 0:\n",
    "              self.car.velocity = Vector(0.5, 0).rotate(self.car.angle)\n",
    "              #print(1, goal_x, goal_y, distance, int(self.car.x),int(self.car.y), im.read_pixel(int(self.car.x),int(self.car.y)))\n",
    "\n",
    "              reward = -1\n",
    "          else: # otherwise\n",
    "              self.car.velocity = Vector(2, 0).rotate(self.car.angle)\n",
    "              reward = -0.2\n",
    "              #print(0, goal_x, goal_y, distance, int(self.car.x),int(self.car.y), im.read_pixel(int(self.car.x),int(self.car.y)))\n",
    "              if distance < last_distance:\n",
    "                  reward = 0.1\n",
    "              # else:\n",
    "              #     last_reward = last_reward +(-0.2)\n",
    "\n",
    "\n",
    "\n",
    "          if self.car.x < 5:\n",
    "              self.car.x = 5\n",
    "              reward = -1\n",
    "          if self.car.x > self.width - 5:\n",
    "              self.car.x = self.width - 5\n",
    "              reward = -1\n",
    "          if self.car.y < 5:\n",
    "              self.car.y = 5\n",
    "              reward = -1\n",
    "          if self.car.y > self.height - 5:\n",
    "              self.car.y = self.height - 5\n",
    "              reward = -1\n",
    "\n",
    "          if distance < 25:\n",
    "              if swap == 1:\n",
    "                  goal_x = 1420\n",
    "                  goal_y = 622\n",
    "                  swap = 0\n",
    "              else:\n",
    "                  goal_x = 9\n",
    "                  goal_y = 85\n",
    "                  swap = 1\n",
    "          last_distance = distance\n",
    "\n",
    "          # cnn state calculation   ### NEW\n",
    "          _,_,new_obs = get_target_image(main_img, self.car.angle, [self.car.x, self.car.y], image_size)\n",
    "          xx = goal_x - self.car.x\n",
    "          yy = goal_y - self.car.y\n",
    "          new_orientation = Vector(*self.car.velocity).angle((xx,yy))/180.\n",
    "          new_orientation = [new_orientation, -new_orientation]\n",
    "          # save_cropped_image(new_obs, self.car.x, self.car.y, name = \"\")\n",
    "          # cnn state calculation  ### NEW\n",
    "\n",
    "          # state calculation\n",
    "          # xx = goal_x - self.car.x\n",
    "          # yy = goal_y - self.car.y\n",
    "          # orientation = Vector(*self.car.velocity).angle((xx,yy))/180.\n",
    "          # new_obs = [self.car.signal1, self.car.signal2, self.car.signal3, orientation, -orientation]\n",
    "          # state calculation\n",
    "\n",
    "          reward_window.append(reward)\n",
    "\n",
    "          if sum(reward_window[len(reward_window)-40:]) <= -38 or episode_timesteps % 2500 == 0 and episode_timesteps != 0:\n",
    "              done = True\n",
    "              reward_window = []\n",
    "\n",
    "\n",
    "          # ENV STEP PERFORM END\n",
    "\n",
    "         # new_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "\n",
    "\n",
    "          # We check if the episode is done\n",
    "          #done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
    "\n",
    "          # We increase the total reward\n",
    "          episode_reward += reward\n",
    "\n",
    "          # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "          replay_buffer.add((obs, orientation, new_obs, new_orientation, action, reward, done))\n",
    "\n",
    "          # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
    "          obs = new_obs\n",
    "          orientation = new_orientation\n",
    "          episode_timesteps += 1\n",
    "          total_timesteps += 1\n",
    "          timesteps_since_eval += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Adding the painting tools\n",
    "\n",
    "class MyPaintWidget(Widget):\n",
    "\n",
    "    def on_touch_down(self, touch):\n",
    "        global length, n_points, last_x, last_y\n",
    "        with self.canvas:\n",
    "            Color(0.8,0.7,0)\n",
    "            d = 10.\n",
    "            touch.ud['line'] = Line(points = (touch.x, touch.y), width = 10)\n",
    "            last_x = int(touch.x)\n",
    "            last_y = int(touch.y)\n",
    "            n_points = 0\n",
    "            length = 0\n",
    "            sand[int(touch.x),int(touch.y)] = 1\n",
    "            img = PILImage.fromarray(sand.astype(\"uint8\")*255)\n",
    "            img.save(\"./images/sand.jpg\")\n",
    "\n",
    "    def on_touch_move(self, touch):\n",
    "        global length, n_points, last_x, last_y\n",
    "        if touch.button == 'left':\n",
    "            touch.ud['line'].points += [touch.x, touch.y]\n",
    "            x = int(touch.x)\n",
    "            y = int(touch.y)\n",
    "            length += np.sqrt(max((x - last_x)**2 + (y - last_y)**2, 2))\n",
    "            n_points += 1.\n",
    "            density = n_points/(length)\n",
    "            touch.ud['line'].width = int(20 * density + 1)\n",
    "            sand[int(touch.x) - 10 : int(touch.x) + 10, int(touch.y) - 10 : int(touch.y) + 10] = 1\n",
    "\n",
    "\n",
    "            last_x = x\n",
    "            last_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the API Buttons (clear, save and load)\n",
    "\n",
    "class CarApp(App):\n",
    "\n",
    "    def build(self):\n",
    "        parent = Game()\n",
    "        parent.serve_car()\n",
    "        Clock.schedule_interval(parent.update, 1.0/60.0)\n",
    "        self.painter = MyPaintWidget()\n",
    "        clearbtn = Button(text = 'clear')\n",
    "        savebtn = Button(text = 'save', pos = (parent.width, 0))\n",
    "        loadbtn = Button(text = 'load', pos = (2 * parent.width, 0))\n",
    "        clearbtn.bind(on_release = self.clear_canvas)\n",
    "        savebtn.bind(on_release = self.save)\n",
    "        loadbtn.bind(on_release = self.load)\n",
    "        parent.add_widget(self.painter)\n",
    "        parent.add_widget(clearbtn)\n",
    "        parent.add_widget(savebtn)\n",
    "        parent.add_widget(loadbtn)\n",
    "        return parent\n",
    "\n",
    "    def clear_canvas(self, obj):\n",
    "        global sand\n",
    "        self.painter.canvas.clear()\n",
    "        sand = np.zeros((longueur,largeur))\n",
    "\n",
    "    def save(self, obj):\n",
    "        print(\"saving brain...\")\n",
    "        brain.save()\n",
    "        plt.plot(scores)\n",
    "        plt.show()\n",
    "\n",
    "    def load(self, obj):\n",
    "        print(\"loading last saved brain...\")\n",
    "        brain.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] [Logger      ] Record log in C:\\Users\\abhi\\.kivy\\logs\\kivy_20-04-17_9.txt\n",
      "[INFO   ] [Kivy        ] v1.11.1\n",
      "[INFO   ] [Kivy        ] Installed at \"C:\\Users\\abhi\\Anaconda3\\lib\\site-packages\\kivy\\__init__.py\"\n",
      "[INFO   ] [Python      ] v3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 14:00:49) [MSC v.1915 64 bit (AMD64)]\n",
      "[INFO   ] [Python      ] Interpreter at \"C:\\Users\\abhi\\Anaconda3\\python.exe\"\n",
      "[INFO   ] [Logger      ] Purge log fired. Analysing...\n",
      "[INFO   ] [Logger      ] Purge 1 log files\n",
      "[INFO   ] [Logger      ] Purge finished!\n",
      "[INFO   ] [Factory     ] 184 symbols loaded\n",
      "[INFO   ] [Image       ] Providers: img_tex, img_dds, img_sdl2, img_pil, img_gif (img_ffpyplayer ignored)\n",
      "[INFO   ] [Text        ] Provider: sdl2\n",
      "[INFO   ] [Window      ] Provider: sdl2\n",
      "[INFO   ] [GL          ] Using the \"OpenGL\" graphics system\n",
      "[INFO   ] [GL          ] GLEW initialization succeeded\n",
      "[INFO   ] [GL          ] Backend used <glew>\n",
      "[INFO   ] [GL          ] OpenGL version <b'4.4.0 - Build 21.20.16.4664'>\n",
      "[INFO   ] [GL          ] OpenGL vendor <b'Intel'>\n",
      "[INFO   ] [GL          ] OpenGL renderer <b'Intel(R) HD Graphics 620'>\n",
      "[INFO   ] [GL          ] OpenGL parsed version: 4, 4\n",
      "[INFO   ] [GL          ] Shading version <b'4.40 - Build 21.20.16.4664'>\n",
      "[INFO   ] [GL          ] Texture max size <16384>\n",
      "[INFO   ] [GL          ] Texture max units <32>\n",
      "[INFO   ] [Window      ] auto add sdl2 input provider\n",
      "[INFO   ] [Window      ] virtual keyboard not allowed, single mode, not docked\n",
      "[INFO   ] [GL          ] NPOT texture support is available\n",
      "[INFO   ] [Base        ] Start application main loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 45 Episode Num: 1 Timesteps diff: 44 Reward: -37.9 score: -0.8613636363636363\n",
      "Total Timesteps: 124 Episode Num: 2 Timesteps diff: 79 Reward: -60.79999999999998 score: -0.7696202531645567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] [WindowSDL   ] exiting mainloop and closing.\n",
      "[INFO   ] [Base        ] Leaving application in progress...\n"
     ]
    }
   ],
   "source": [
    "# Running the whole thing\n",
    "if __name__ == '__main__':\n",
    "    CarApp().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
